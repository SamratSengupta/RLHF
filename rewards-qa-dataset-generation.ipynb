{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3398791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"path_to_your_pdf.pdf\"\n",
    "llm = OpenAI(model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfFileReader(file)\n",
    "        for page_num in range(reader.numPages):\n",
    "            page = reader.getPage(page_num)\n",
    "            pdf_text += page.extract_text()\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "pdf_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cadd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "    dtm = vectorizer.fit_transform(text.split(\"\\n\"))\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    return dtm, terms\n",
    "\n",
    "dtm, terms = preprocess_text(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dtm, terms, start=10, step=2, limit=20):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
    "        lda_model.fit(dtm)\n",
    "        model_list.append(lda_model)\n",
    "        \n",
    "        topics = lda_model.components_\n",
    "        topic_words = [[terms[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in topics]\n",
    "        \n",
    "        texts = [terms[idx] for idx in dtm.nonzero()[1]]\n",
    "        dictionary = Dictionary([texts])\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        \n",
    "        coherence_model = CoherenceModel(topics=topic_words, texts=[texts], dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    return model_list, coherence_values\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dtm, terms, start=10, step=2, limit=30)\n",
    "optimal_model = model_list[np.argmax(coherence_values)]\n",
    "optimal_num_topics = 10 + 2 * np.argmax(coherence_values)\n",
    "print(f\"Optimal number of topics: {optimal_num_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777b7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=\"\"\"\n",
    "    Generate three high-quality question-answer pairs based on the following context:\n",
    "    Context: {context}\n",
    "\n",
    "    For each question, provide three levels of answers:\n",
    "    - Thorough understanding\n",
    "    - Partial understanding\n",
    "    - Limited understanding\n",
    "\n",
    "    Q1: \n",
    "    A1 (Thorough, Score 5): \n",
    "    A1 (Good, Score 4): \n",
    "    A1 (Average, Score 3): \n",
    "    A1 (Below Average, Score 2): \n",
    "    A1 (Limited, Score 1): \n",
    "    Q2: \n",
    "    A2 (Thorough, Score 5): \n",
    "    A2 (Good, Score 4): \n",
    "    A2 (Average, Score 3): \n",
    "    A2 (Below Average, Score 2): \n",
    "    A2 (Limited, Score 1): \n",
    "    Q3: \n",
    "    A3 (Thorough, Score 5): \n",
    "    A3 (Good, Score 4): \n",
    "    A3 (Average, Score 3): \n",
    "    A3 (Below Average, Score 2): \n",
    "    A3 (Limited, Score 1): \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create the LLMChain\n",
    "qa_chain = LLMChain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf7132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs(context, num_pairs=3):\n",
    "    qa_text = qa_chain.run(context=context)\n",
    "    qa_pairs = []\n",
    "    for i in range(num_pairs):\n",
    "        question = qa_text.split(f\"Q{i+1}:\")[1].split(f\"A{i+1} (Thorough, Score 5):\")[0].strip()\n",
    "        thorough_answer = qa_text.split(f\"A{i+1} (Thorough, Score 5):\")[1].split(f\"A{i+1} (Good, Score 4):\")[0].strip()\n",
    "        good_answer = qa_text.split(f\"A{i+1} (Good, Score 4):\")[1].split(f\"A{i+1} (Average, Score 3):\")[0].strip()\n",
    "        average_answer = qa_text.split(f\"A{i+1} (Average, Score 3):\")[1].split(f\"A{i+1} (Below Average, Score 2):\")[0].strip()\n",
    "        below_avg_answer = qa_text.split(f\"A{i+1} (Below Average, Score 2):\")[1].split(f\"A{i+1} (Limited, Score 1):\")[0].strip()\n",
    "        limited_answer = qa_text.split(f\"A{i+1} (Limited, Score 1):\")[1].split(f\"Q{i+2}:\")[0].strip() if i+2 <= num_pairs else qa_text.split(f\"A{i+1} (Limited, Score 1):\")[1].strip()\n",
    "        qa_pairs.append({\n",
    "            \"question\": question,\n",
    "            \"answers\": [\n",
    "                {\"text\": thorough_answer, \"score\": 5},\n",
    "                {\"text\": good_answer, \"score\": 4},\n",
    "                {\"text\": average_answer, \"score\": 3},\n",
    "                {\"text\": below_avg_answer, \"score\": 2},\n",
    "                {\"text\": limited_answer, \"score\": 1}\n",
    "            ]\n",
    "        })\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_dataset(pdf_text, lda_model, terms, n_questions_per_topic=3, total_questions=1200):\n",
    "    topics = lda_model.components_\n",
    "    all_qa_pairs = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        topic_words = [terms[i] for i in topic.argsort()[:-n_questions_per_topic - 1:-1]]\n",
    "        context_sentences = [sentence for sentence in pdf_text.split(\". \") if any(word in sentence for word in topic_words)]\n",
    "        context = \". \".join(context_sentences[:10])  # Limit context size for better generation quality\n",
    "        qa_pairs = generate_qa_pairs(context, num_pairs=n_questions_per_topic)\n",
    "        all_qa_pairs.extend(qa_pairs)\n",
    "        if len(all_qa_pairs) >= total_questions:\n",
    "            break\n",
    "    \n",
    "    return all_qa_pairs[:total_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f42f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_dataset = create_qa_dataset(pdf_text, optimal_model, terms)\n",
    "print(qa_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
