{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import numpy as np\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"path_to_your_pdf.pdf\"\n",
    "llm = OpenAI(model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14758c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfFileReader(file)\n",
    "        for page_num in range(reader.numPages):\n",
    "            page = reader.getPage(page_num)\n",
    "            pdf_text += page.extract_text()\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "pdf_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280949f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "    dtm = vectorizer.fit_transform(text.split(\"\\n\"))\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    return dtm, terms\n",
    "\n",
    "dtm, terms = preprocess_text(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dtm, terms, start=10, step=2, limit=30):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
    "        lda_model.fit(dtm)\n",
    "        model_list.append(lda_model)\n",
    "        \n",
    "        topics = lda_model.components_\n",
    "        topic_words = [[terms[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in topics]\n",
    "        \n",
    "        texts = [terms[idx] for idx in dtm.nonzero()[1]]\n",
    "        dictionary = Dictionary([texts])\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        \n",
    "        coherence_model = CoherenceModel(topics=topic_words, texts=[texts], dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dtm, terms, start=10, step=2, limit=30)\n",
    "optimal_model = model_list[np.argmax(coherence_values)]\n",
    "optimal_num_topics = 10 + 2 * np.argmax(coherence_values)\n",
    "print(f\"Optimal number of topics: {optimal_num_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"num_pairs\"],\n",
    "    template=\"\"\"\n",
    "    Based on the following context, generate {num_pairs} high-quality question-answer pairs. Ensure the questions are relevant and the answers are detailed.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    {questions_and_answers}\n",
    "    \"\"\"\n",
    ")\n",
    "# Create the LLMChain\n",
    "qa_chain = LLMChain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ce13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_qa_pairs(context, num_pairs):\n",
    "    questions_and_answers = \"\\n\".join([f\"Q{i+1}: \\nA{i+1}: \\n\" for i in range(num_pairs)])\n",
    "    qa_text = qa_chain.run(context=context, num_pairs=num_pairs, questions_and_answers=questions_and_answers)\n",
    "    qa_pairs = []\n",
    "    for i in range(num_pairs):\n",
    "        question = qa_text.split(f\"Q{i+1}:\")[1].split(f\"A{i+1}:\")[0].strip()\n",
    "        answer = qa_text.split(f\"A{i+1}:\")[1].split(f\"Q{i+2}:\")[0].strip() if i+2 <= num_pairs else qa_text.split(f\"A{i+1}:\")[1].strip()\n",
    "        qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e497fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_dataset(pdf_text, lda_model, terms, total_questions=1200):\n",
    "    topics = lda_model.components_\n",
    "    num_topics = len(topics)\n",
    "    n_questions_per_topic = total_questions // num_topics\n",
    "    num_pairs_per_call = 10  # Adjust this number to optimize the LLM calls\n",
    "    all_qa_pairs = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        topic_words = [terms[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "        context_sentences = [sentence for sentence in pdf_text.split(\". \") if any(word in sentence for word in topic_words)]\n",
    "        context = \". \".join(context_sentences[:10])  # Limit context size for better generation quality\n",
    "        \n",
    "        for _ in range(n_questions_per_topic // num_pairs_per_call):\n",
    "            qa_pairs = generate_multiple_qa_pairs(context, num_pairs_per_call)\n",
    "            all_qa_pairs.extend(qa_pairs)\n",
    "            if len(all_qa_pairs) >= total_questions:\n",
    "                break\n",
    "        if len(all_qa_pairs) >= total_questions:\n",
    "            break\n",
    "    \n",
    "    return all_qa_pairs[:total_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65498006",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = create_qa_dataset(pdf_text, optimal_model, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b2856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filename, directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    with open(filepath, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(qa_dataset, 'qa_dataset.json', 'qa_datasets')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
